# -*- coding: utf-8 -*-
"""Finance News Analysis_Word2Vec.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_vsjmmHaXNrf8WJuDUyT903Sfzr7ku8w
"""

#importing the library
import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt

import pandas as pd
import numpy as np
import nltk
nltk.download('punkt') # Used for sentence tokenizer
nltk.download('stopwords')
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

df = pd.read_csv('/content/sample_data/all-data.csv', encoding='latin-1', names=['Sentiment', 'Text'])

# Display the DataFrame
df.head()

# @title Sentiment

from matplotlib import pyplot as plt
import seaborn as sns
df.groupby('Sentiment').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

"""Check Missing Value"""

df.info()

df['Sentiment'].value_counts().plot(kind='bar')

df['Sentiment'].value_counts().plot(kind='pie')

"""Preprocessing Data"""

text_lines = list()
lines = df['Text'].values.tolist()
for line in lines:
    tokens = word_tokenize(line)
    # convert to lower case
    tokens = [w.lower() for w in tokens]
    # remove punctuation from each word
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]
    # remove remaining tokens that are not alphabetic
    words = [word for word in stripped if word.isalpha()]
    # filter out stop words
    stop_words = set(stopwords.words('english'))
    words = [w for w in words if not w in stop_words]
    text_lines.append(words)

len(text_lines)
df.shape

"""Word2Vec"""

import gensim # Process plain text
EMBEDDING_DIM = 100
# train word2vec model
model = gensim.models.Word2Vec(sentences=text_lines, vector_size=EMBEDDING_DIM, window=5, min_count=1)
# vocab size
words = list(model.wv.key_to_index) # wv is the object that contain mappings between words and embeddings
print('Vocabulary size: %d' % len(words))

words

# save model in ASCII (word2vec) format
filename = 'imdb_embedding_word2vec.txt'
model.wv.save_word2vec_format(filename, binary=False)

"""# NLP"""

def get_sequences(text):
    #creating a tokenizer object
    tokenizer=Tokenizer()
    #using fit_on_text method to convert word into number most frequent would assign to 1 and with lower frequency assign to lower number
    tokenizer.fit_on_texts(text)
    #getting the word and the number assigning to them
    sequences=tokenizer.texts_to_sequences(text)


    #getting maximum length of list inthe sequences list

    print('Maximum Vocab',len(tokenizer.word_index))
    max_sequence_length=np.max(list(map(lambda x:len(x),sequences)))

    print('Max Sequences Length',max_sequence_length)

    sequences=pad_sequences(sequences,maxlen=max_sequence_length,padding='post')



    return sequences

get_sequences(df['Text'])

def preprocess_inputs(df):
    df=df.copy()
    sequences=get_sequences(df['Text'])
    df['Sentiment']=df['Sentiment'].replace({'negative':0,'positive':1,'neutral':2})
    y=df['Sentiment']

    x_train,x_test,y_train,y_test=train_test_split(sequences,y,train_size=0.7,random_state=1)
    return x_train,x_test,y_train,y_test

x_train,x_test,y_train,y_test=preprocess_inputs(df)
x_train

x_train.shape

y_train

"""Finding out similiar words"""

model.wv.most_similar('man', topn =5) #top 5

"""Constructing the Neural Network"""

inputs=tf.keras.Input(shape=(71,))
x=tf.keras.layers.Embedding(
input_dim=10123,
output_dim=128,
input_length=71)(inputs)
x=tf.keras.layers.GRU(256,activation='tanh')(x)
outputs=tf.keras.layers.Dense(3,activation='softmax')(x)
model=tf.keras.Model(inputs=inputs,outputs=outputs)
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history=model.fit(x_train,y_train,validation_split=0.2,
                 batch_size=32,epochs=100,
                 callbacks=[tf.keras.callbacks.EarlyStopping(
                 monitor='val_loss',
                 patience=3,
                 restore_best_weights=True)])

model.evaluate(x_test,y_test)

x_test

model.predict(x_test)

y_pred=np.argmax(model.predict(x_test),axis=1)

from sklearn.metrics import confusion_matrix

cm=confusion_matrix(np.array(y_test),y_pred)
sns.heatmap(cm,annot=True,cmap='mako')
plt.show()

"""# Back to the Top

Finding the odd word out :
"""

print(model.wv.doesnt_match("finance".split()))
print(model.wv.doesnt_match("business holding".split()))

"""Similarity between words :"""

print(model.wv.similarity('finance', 'business'))

"""### Creating a dictionary to store the word and itâ€™s corresponding embedding"""

embeddings_index = {}
f = open('/content/imdb_embedding_word2vec.txt')
for line in f:
    print(line)
    values = line.split()
    word = values[0] # Since the word is at the 0th index
    coefs = np.asarray(values[1:]) # From the 1st position onwards the values are the embedding values
    embeddings_index[word] = coefs
f.close()

"""### Tokenization and Padding :"""

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
VALIDATION_SPLIT = 0.2
tokenizer_obj = Tokenizer()
tokenizer_obj.fit_on_texts(text_lines)
sequences = tokenizer_obj.texts_to_sequences(text_lines) # Transforms text into a sequence of integers

max_length = 100 # Maximum length of sentences
# pad sequences
word_index = tokenizer_obj.word_index # Maps words to their numeric representation
print('Found %s unique tokens.' % len(word_index))
text_pad = pad_sequences(sequences, maxlen=max_length)
sentiment =  df['Sentiment'].values
print('Shape of review tensor:', text_pad.shape)
print('Shape of sentiment tensor:', sentiment.shape)

tokenizer_obj.index_word

tokenizer_obj.word_index

text_pad[2000]

"""### Shuffling the dataset and splitting into train and validation"""

# split the data into a training set and a validation set
indices = np.arange(text_pad.shape[0])
print(indices)
np.random.shuffle(indices)
print(indices)

text_pad = text_pad[indices]
sentiment = sentiment[indices]
num_validation_samples = int(VALIDATION_SPLIT * text_pad.shape[0])

X_train_pad = text_pad[:-num_validation_samples]
y_train = sentiment[:-num_validation_samples]
X_test_pad = text_pad[-num_validation_samples:]
y_test = sentiment[-num_validation_samples:]

print('Shape of Training dataset X:', X_train_pad.shape)
print('Shape of Training dataset Y:', y_train.shape)
print('Shape of Validation dataset X:', X_test_pad.shape)
print('Shape of Valodation daatset Y:', y_test.shape)

"""Generating the Embedding Matrix from the trained word embeddings"""

EMBEDDING_DIM =100
num_words = len(word_index) + 1
embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
# words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

"""### Define the model"""

from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, GRU
from keras.initializers import Constant
# define model
model = Sequential()
embedding_layer = Embedding(num_words, EMBEDDING_DIM, embeddings_initializer=Constant(embedding_matrix), input_length=max_length, trainable=False)
model.add(embedding_layer)
model.add(LSTM(units=32,  dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print('Summary of the built model...')
print(model.summary())

"""###Train the model"""

print(y_train[:5])  # Check the first few training labels
print(y_test[:5])   # Check the first few test labels

model.summary()  # Check the model architecture

from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense

# Define the model
model = Sequential()
model.add(Embedding(input_dim=9230, output_dim=100, input_length=100))  # Adjust input_dim as needed
model.add(LSTM(32))
model.add(Dense(units=4, activation='softmax'))  # Assuming 4 classes: 'negative', 'positive', 'neutral', and another class

# Compile the model with the appropriate loss function
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Model summary
model.summary()

print(X_train_pad.dtype, y_train.dtype)
print(X_test_pad.dtype, y_test.dtype)

from sklearn.preprocessing import LabelEncoder

# Convert string labels to integers
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Compile the model with the correct loss function
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Fit the model with the encoded labels
model.fit(X_train_pad, y_train_encoded, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test_encoded), verbose=2)

"""Save the model architecture and weights"""

model.save_weights('Sentiment_Classifier_word2vec_first_try.h5')

# We save the weights to a h5 file.
with open('model_architecture_Sentiment_classifier_word2vec_first_try.json','w') as f:
    f.write(model.to_json())

"""### Testing the Model"""

from keras.models import load_model
from keras.models import model_from_json

# Model reconstruction from JSON file
with open('/content/model_architecture_Sentiment_classifier_word2vec_first_try.json', 'r') as f:
    model = model_from_json(f.read())
model.load_weights('/content/Sentiment_Classifier_word2vec_first_try.h5')

from sklearn.preprocessing import LabelEncoder

# Convert string labels to integers
label_encoder = LabelEncoder()
y_test_encoded = label_encoder.fit_transform(y_test)

# Compile the model with the correct loss function for multi-class classification
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Evaluate the model with the encoded labels
print('Testing...')
score, acc = model.evaluate(X_test_pad, y_test_encoded, batch_size=128)
print('Test score:', score)
print('Test accuracy:', acc)
print("Accuracy: {0:.2%}".format(acc))